{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load DBN.py\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams\n",
    "\n",
    "from logistic_sgd import LogisticRegression, load_data\n",
    "from mlp import HiddenLayer\n",
    "from rbm import RBM\n",
    "\n",
    "\n",
    "# start-snippet-1\n",
    "class DBN(object):\n",
    "    \"\"\"Deep Belief Network\n",
    "\n",
    "    A deep belief network is obtained by stacking several RBMs on top of each\n",
    "    other. The hidden layer of the RBM at layer `i` becomes the input of the\n",
    "    RBM at layer `i+1`. The first layer RBM gets as input the input of the\n",
    "    network, and the hidden layer of the last RBM represents the output. When\n",
    "    used for classification, the DBN is treated as a MLP, by adding a logistic\n",
    "    regression layer on top.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, numpy_rng, theano_rng=None, n_ins=784,\n",
    "                 hidden_layers_sizes=[500, 500], n_outs=10):\n",
    "        \"\"\"This class is made to support a variable number of layers.\n",
    "\n",
    "        :type numpy_rng: numpy.random.RandomState\n",
    "        :param numpy_rng: numpy random number generator used to draw initial\n",
    "                    weights\n",
    "\n",
    "        :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams\n",
    "        :param theano_rng: Theano random generator; if None is given one is\n",
    "                           generated based on a seed drawn from `rng`\n",
    "\n",
    "        :type n_ins: int\n",
    "        :param n_ins: dimension of the input to the DBN\n",
    "\n",
    "        :type hidden_layers_sizes: list of ints\n",
    "        :param hidden_layers_sizes: intermediate layers size, must contain\n",
    "                               at least one value\n",
    "\n",
    "        :type n_outs: int\n",
    "        :param n_outs: dimension of the output of the network\n",
    "        \"\"\"\n",
    "\n",
    "        self.sigmoid_layers = []\n",
    "        self.rbm_layers = []\n",
    "        self.params = []\n",
    "        self.n_layers = len(hidden_layers_sizes)\n",
    "\n",
    "        assert self.n_layers > 0\n",
    "\n",
    "        if not theano_rng:\n",
    "            theano_rng = MRG_RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        # allocate symbolic variables for the data\n",
    "        self.x = T.matrix('x')  # the data is presented as rasterized images\n",
    "        self.y = T.ivector('y')  # the labels are presented as 1D vector\n",
    "                                 # of [int] labels\n",
    "        # end-snippet-1\n",
    "        # The DBN is an MLP, for which all weights of intermediate\n",
    "        # layers are shared with a different RBM.  We will first\n",
    "        # construct the DBN as a deep multilayer perceptron, and when\n",
    "        # constructing each sigmoidal layer we also construct an RBM\n",
    "        # that shares weights with that layer. During pretraining we\n",
    "        # will train these RBMs (which will lead to chainging the\n",
    "        # weights of the MLP as well) During finetuning we will finish\n",
    "        # training the DBN by doing stochastic gradient descent on the\n",
    "        # MLP.\n",
    "\n",
    "        for i in xrange(self.n_layers):\n",
    "            # construct the sigmoidal layer\n",
    "\n",
    "            # the size of the input is either the number of hidden\n",
    "            # units of the layer below or the input size if we are on\n",
    "            # the first layer\n",
    "            if i == 0:\n",
    "                input_size = n_ins\n",
    "            else:\n",
    "                input_size = hidden_layers_sizes[i - 1]\n",
    "\n",
    "            # the input to this layer is either the activation of the\n",
    "            # hidden layer below or the input of the DBN if you are on\n",
    "            # the first layer\n",
    "            if i == 0:\n",
    "                layer_input = self.x\n",
    "            else:\n",
    "                layer_input = self.sigmoid_layers[-1].output\n",
    "\n",
    "            sigmoid_layer = HiddenLayer(rng=numpy_rng,\n",
    "                                        input=layer_input,\n",
    "                                        n_in=input_size,\n",
    "                                        n_out=hidden_layers_sizes[i],\n",
    "                                        activation=T.nnet.sigmoid)\n",
    "\n",
    "            # add the layer to our list of layers\n",
    "            self.sigmoid_layers.append(sigmoid_layer)\n",
    "\n",
    "            # its arguably a philosophical question...  but we are\n",
    "            # going to only declare that the parameters of the\n",
    "            # sigmoid_layers are parameters of the DBN. The visible\n",
    "            # biases in the RBM are parameters of those RBMs, but not\n",
    "            # of the DBN.\n",
    "            self.params.extend(sigmoid_layer.params)\n",
    "\n",
    "            # Construct an RBM that shared weights with this layer\n",
    "            rbm_layer = RBM(numpy_rng=numpy_rng,\n",
    "                            theano_rng=theano_rng,\n",
    "                            input=layer_input,\n",
    "                            n_visible=input_size,\n",
    "                            n_hidden=hidden_layers_sizes[i],\n",
    "                            W=sigmoid_layer.W,\n",
    "                            hbias=sigmoid_layer.b)\n",
    "            self.rbm_layers.append(rbm_layer)\n",
    "\n",
    "        # We now need to add a logistic layer on top of the MLP\n",
    "        self.logLayer = LogisticRegression(\n",
    "            input=self.sigmoid_layers[-1].output,\n",
    "            n_in=hidden_layers_sizes[-1],\n",
    "            n_out=n_outs)\n",
    "        self.params.extend(self.logLayer.params)\n",
    "\n",
    "        # compute the cost for second phase of training, defined as the\n",
    "        # negative log likelihood of the logistic regression (output) layer\n",
    "        self.finetune_cost = self.logLayer.negative_log_likelihood(self.y)\n",
    "\n",
    "        # compute the gradients with respect to the model parameters\n",
    "        # symbolic variable that points to the number of errors made on the\n",
    "        # minibatch given by self.x and self.y\n",
    "        self.errors = self.logLayer.errors(self.y)\n",
    "\n",
    "    def pretraining_functions(self, train_set_x, batch_size, k):\n",
    "        '''Generates a list of functions, for performing one step of\n",
    "        gradient descent at a given layer. The function will require\n",
    "        as input the minibatch index, and to train an RBM you just\n",
    "        need to iterate, calling the corresponding function on all\n",
    "        minibatch indexes.\n",
    "\n",
    "        :type train_set_x: theano.tensor.TensorType\n",
    "        :param train_set_x: Shared var. that contains all datapoints used\n",
    "                            for training the RBM\n",
    "        :type batch_size: int\n",
    "        :param batch_size: size of a [mini]batch\n",
    "        :param k: number of Gibbs steps to do in CD-k / PCD-k\n",
    "\n",
    "        '''\n",
    "\n",
    "        # index to a [mini]batch\n",
    "        index = T.lscalar('index')  # index to a minibatch\n",
    "        learning_rate = T.scalar('lr')  # learning rate to use\n",
    "\n",
    "        # number of batches\n",
    "        n_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "        # begining of a batch, given `index`\n",
    "        batch_begin = index * batch_size\n",
    "        # ending of a batch given `index`\n",
    "        batch_end = batch_begin + batch_size\n",
    "\n",
    "        pretrain_fns = []\n",
    "        for rbm in self.rbm_layers:\n",
    "\n",
    "            # get the cost and the updates list\n",
    "            # using CD-k here (persisent=None) for training each RBM.\n",
    "            # TODO: change cost function to reconstruction error\n",
    "            cost, updates = rbm.get_cost_updates(learning_rate,\n",
    "                                                 persistent=None, k=k)\n",
    "\n",
    "            # compile the theano function\n",
    "            fn = theano.function(\n",
    "                inputs=[index, theano.Param(learning_rate, default=0.1)],\n",
    "                outputs=cost,\n",
    "                updates=updates,\n",
    "                givens={\n",
    "                    self.x: train_set_x[batch_begin:batch_end]\n",
    "                }\n",
    "            )\n",
    "            # append `fn` to the list of functions\n",
    "            pretrain_fns.append(fn)\n",
    "\n",
    "        return pretrain_fns\n",
    "\n",
    "    def build_finetune_functions(self, datasets, batch_size, learning_rate):\n",
    "        '''Generates a function `train` that implements one step of\n",
    "        finetuning, a function `validate` that computes the error on a\n",
    "        batch from the validation set, and a function `test` that\n",
    "        computes the error on a batch from the testing set\n",
    "\n",
    "        :type datasets: list of pairs of theano.tensor.TensorType\n",
    "        :param datasets: It is a list that contain all the datasets;\n",
    "                        the has to contain three pairs, `train`,\n",
    "                        `valid`, `test` in this order, where each pair\n",
    "                        is formed of two Theano variables, one for the\n",
    "                        datapoints, the other for the labels\n",
    "        :type batch_size: int\n",
    "        :param batch_size: size of a minibatch\n",
    "        :type learning_rate: float\n",
    "        :param learning_rate: learning rate used during finetune stage\n",
    "\n",
    "        '''\n",
    "\n",
    "        (train_set_x, train_set_y) = datasets[0]\n",
    "        (valid_set_x, valid_set_y) = datasets[1]\n",
    "        (test_set_x, test_set_y) = datasets[2]\n",
    "\n",
    "        # compute number of minibatches for training, validation and testing\n",
    "        n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
    "        n_valid_batches /= batch_size\n",
    "        n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
    "        n_test_batches /= batch_size\n",
    "\n",
    "        index = T.lscalar('index')  # index to a [mini]batch\n",
    "\n",
    "        # compute the gradients with respect to the model parameters\n",
    "        gparams = T.grad(self.finetune_cost, self.params)\n",
    "\n",
    "        # compute list of fine-tuning updates\n",
    "        updates = []\n",
    "        for param, gparam in zip(self.params, gparams):\n",
    "            updates.append((param, param - gparam * learning_rate))\n",
    "\n",
    "        train_fn = theano.function(\n",
    "            inputs=[index],\n",
    "            outputs=self.finetune_cost,\n",
    "            updates=updates,\n",
    "            givens={\n",
    "                self.x: train_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: train_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        test_score_i = theano.function(\n",
    "            [index],\n",
    "            self.errors,\n",
    "            givens={\n",
    "                self.x: test_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: test_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        valid_score_i = theano.function(\n",
    "            [index],\n",
    "            self.errors,\n",
    "            givens={\n",
    "                self.x: valid_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: valid_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Create a function that scans the entire validation set\n",
    "        def valid_score():\n",
    "            return [valid_score_i(i) for i in xrange(n_valid_batches)]\n",
    "\n",
    "        # Create a function that scans the entire test set\n",
    "        def test_score():\n",
    "            return [test_score_i(i) for i in xrange(n_test_batches)]\n",
    "\n",
    "        return train_fn, valid_score, test_score\n",
    "\n",
    "\n",
    "def test_DBN(finetune_lr=0.1, pretraining_epochs=100,\n",
    "             pretrain_lr=0.01, k=1, training_epochs=1000,\n",
    "             dataset='mnist.pkl.gz', batch_size=10):\n",
    "    \"\"\"\n",
    "    Demonstrates how to train and test a Deep Belief Network.\n",
    "\n",
    "    This is demonstrated on MNIST.\n",
    "\n",
    "    :type finetune_lr: float\n",
    "    :param finetune_lr: learning rate used in the finetune stage\n",
    "    :type pretraining_epochs: int\n",
    "    :param pretraining_epochs: number of epoch to do pretraining\n",
    "    :type pretrain_lr: float\n",
    "    :param pretrain_lr: learning rate to be used during pre-training\n",
    "    :type k: int\n",
    "    :param k: number of Gibbs steps in CD/PCD\n",
    "    :type training_epochs: int\n",
    "    :param training_epochs: maximal number of iterations ot run the optimizer\n",
    "    :type dataset: string\n",
    "    :param dataset: path the the pickled dataset\n",
    "    :type batch_size: int\n",
    "    :param batch_size: the size of a minibatch\n",
    "    \"\"\"\n",
    "\n",
    "    datasets = load_data(dataset)\n",
    "\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "    # numpy random generator\n",
    "    numpy_rng = numpy.random.RandomState(123)\n",
    "    print '... building the model'\n",
    "    # construct the Deep Belief Network\n",
    "    dbn = DBN(numpy_rng=numpy_rng, n_ins=28 * 28,\n",
    "              hidden_layers_sizes=[1000, 1000, 1000],\n",
    "              n_outs=10)\n",
    "\n",
    "    # start-snippet-2\n",
    "    #########################\n",
    "    # PRETRAINING THE MODEL #\n",
    "    #########################\n",
    "    print '... getting the pretraining functions'\n",
    "    pretraining_fns = dbn.pretraining_functions(train_set_x=train_set_x,\n",
    "                                                batch_size=batch_size,\n",
    "                                                k=k)\n",
    "\n",
    "    print '... pre-training the model'\n",
    "    start_time = timeit.default_timer()\n",
    "    ## Pre-train layer-wise\n",
    "    for i in xrange(dbn.n_layers):\n",
    "        # go through pretraining epochs\n",
    "        for epoch in xrange(pretraining_epochs):\n",
    "            # go through the training set\n",
    "            c = []\n",
    "            for batch_index in xrange(n_train_batches):\n",
    "                c.append(pretraining_fns[i](index=batch_index,\n",
    "                                            lr=pretrain_lr))\n",
    "            print 'Pre-training layer %i, epoch %d, cost ' % (i, epoch),\n",
    "            print numpy.mean(c)\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    # end-snippet-2\n",
    "    print >> sys.stderr, ('The pretraining code for file ' +\n",
    "                          os.path.split(__file__)[1] +\n",
    "                          ' ran for %.2fm' % ((end_time - start_time) / 60.))\n",
    "    ########################\n",
    "    # FINETUNING THE MODEL #\n",
    "    ########################\n",
    "\n",
    "    # get the training, validation and testing function for the model\n",
    "    print '... getting the finetuning functions'\n",
    "    train_fn, validate_model, test_model = dbn.build_finetune_functions(\n",
    "        datasets=datasets,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=finetune_lr\n",
    "    )\n",
    "\n",
    "    print '... finetuning the model'\n",
    "    # early-stopping parameters\n",
    "    patience = 4 * n_train_batches  # look as this many examples regardless\n",
    "    patience_increase = 2.    # wait this much longer when a new best is\n",
    "                              # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                   # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience / 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatches before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "\n",
    "    while (epoch < training_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in xrange(n_train_batches):\n",
    "\n",
    "            minibatch_avg_cost = train_fn(minibatch_index)\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "\n",
    "                validation_losses = validate_model()\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "                print(\n",
    "                    'epoch %i, minibatch %i/%i, validation error %f %%'\n",
    "                    % (\n",
    "                        epoch,\n",
    "                        minibatch_index + 1,\n",
    "                        n_train_batches,\n",
    "                        this_validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if (\n",
    "                        this_validation_loss < best_validation_loss *\n",
    "                        improvement_threshold\n",
    "                    ):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = test_model()\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f %%') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                           test_score * 100.))\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    print(\n",
    "        (\n",
    "            'Optimization complete with best validation score of %f %%, '\n",
    "            'obtained at iteration %i, '\n",
    "            'with test performance %f %%'\n",
    "        ) % (best_validation_loss * 100., best_iter + 1, test_score * 100.)\n",
    "    )\n",
    "    print >> sys.stderr, ('The fine tuning code for file ' +\n",
    "                          os.path.split(__file__)[1] +\n",
    "                          ' ran for %.2fm' % ((end_time - start_time)\n",
    "                                              / 60.))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_DBN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n",
      "... building the model\n",
      "... getting the pretraining functions"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X\n",
      "DEBUG: nvcc STDOUT mod.cu\n",
      "   C:/Users/nuri/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.9-64/tmpceevnu/cba162c1c9d8bc4ee70d76f3773ebcee.lib ���̺귯�� �� C:/Users/nuri/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.9-64/tmpceevnu/cba162c1c9d8bc4ee70d76f3773ebcee.exp ��ü�� �����ϰ� �ֽ��ϴ�.\n",
      "\n",
      "DEBUG: nvcc STDOUT mod.cu\n",
      "   C:/Users/nuri/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.9-64/tmpy3four/1aeb2c35baa6f90ae4c07abcc2c639ef.lib ���̺귯�� �� C:/Users/nuri/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.9-64/tmpy3four/1aeb2c35baa6f90ae4c07abcc2c639ef.exp ��ü�� �����ϰ� �ֽ��ϴ�.\n",
      "\n",
      "DEBUG: nvcc STDOUT mod.cu\n",
      "   C:/Users/nuri/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.9-64/tmpqe0ykb/42f604a045b47b470a5afb961c5f126e.lib ���̺귯�� �� C:/Users/nuri/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.9-64/tmpqe0ykb/42f604a045b47b470a5afb961c5f126e.exp ��ü�� �����ϰ� �ֽ��ϴ�.\n",
      "\n",
      "DEBUG: nvcc STDOUT mod.cu\n",
      "   C:/Users/nuri/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.9-64/tmpmeh05w/ce9e7811c2193abd00f259a2d2080af0.lib ���̺귯�� �� C:/Users/nuri/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.9-64/tmpmeh05w/ce9e7811c2193abd00f259a2d2080af0.exp ��ü�� �����ϰ� �ֽ��ϴ�.\n",
      "\n",
      "DEBUG: nvcc STDOUT mod.cu\n",
      "   C:/Users/nuri/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.9-64/tmp7qbqbi/ec0435127f9e499fc8f2019bda4483b8.lib ���̺귯�� �� C:/Users/nuri/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.9-64/tmp7qbqbi/ec0435127f9e499fc8f2019bda4483b8.exp ��ü�� �����ϰ� �ֽ��ϴ�.\n",
      "\n",
      "DEBUG: nvcc STDOUT mod.cu\n",
      "   C:/Users/nuri/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.9-64/tmposxja_/1e4c54d56d89f4fa95c1443c6b760d71.lib ���̺귯�� �� C:/Users/nuri/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.9-64/tmposxja_/1e4c54d56d89f4fa95c1443c6b760d71.exp ��ü�� �����ϰ� �ֽ��ϴ�.\n",
      "\n",
      "DEBUG: nvcc STDOUT mod.cu\n",
      "   C:/Users/nuri/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.9-64/tmpvuko4u/8d1e8a17b4e9e916500e00fa225ba278.lib ���̺귯�� �� C:/Users/nuri/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.9-64/tmpvuko4u/8d1e8a17b4e9e916500e00fa225ba278.exp ��ü�� �����ϰ� �ֽ��ϴ�.\n",
      "\n",
      "DEBUG: nvcc STDOUT"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... pre-training the model\n",
      "Pre-training layer 0, epoch 0, cost  -98.5322\n",
      "Pre-training layer 0, epoch 1, cost  -83.8368\n",
      "Pre-training layer 0, epoch 2, cost  -80.6902\n",
      "Pre-training layer 0, epoch 3, cost  -79.0468\n",
      "Pre-training layer 0, epoch 4, cost  -77.9314\n",
      "Pre-training layer 0, epoch 5, cost  -77.1008\n",
      "Pre-training layer 0, epoch 6, cost  -76.4179\n",
      "Pre-training layer 0, epoch 7, cost  -75.828\n",
      "Pre-training layer 0, epoch 8, cost  -75.3451\n",
      "Pre-training layer 0, epoch 9, cost  -74.9261\n",
      "Pre-training layer 0, epoch 10, cost  -74.6042\n",
      "Pre-training layer 0, epoch 11, cost  -74.2678\n",
      "Pre-training layer 0, epoch 12, cost  -73.9228\n",
      "Pre-training layer 0, epoch 13, cost  -73.6817\n",
      "Pre-training layer 0, epoch 14, cost  -73.4275\n",
      "Pre-training layer 0, epoch 15, cost  -73.1666\n",
      "Pre-training layer 0, epoch 16, cost  -72.962\n",
      "Pre-training layer 0, epoch 17, cost  -72.7905\n",
      "Pre-training layer 0, epoch 18, cost  -72.5986\n",
      "Pre-training layer 0, epoch 19, cost  -72.4591\n",
      "Pre-training layer 0, epoch 20, cost  -72.2536\n",
      "Pre-training layer 0, epoch 21, cost  -72.1634\n",
      "Pre-training layer 0, epoch 22, cost  -71.9912\n",
      "Pre-training layer 0, epoch 23, cost  -71.8198\n",
      "Pre-training layer 0, epoch 24, cost  -71.7223\n",
      "Pre-training layer 0, epoch 25, cost  -71.5692\n",
      "Pre-training layer 0, epoch 26, cost  -71.4509\n",
      "Pre-training layer 0, epoch 27, cost  -71.3551\n",
      "Pre-training layer 0, epoch 28, cost  -71.247\n",
      "Pre-training layer 0, epoch 29, cost  -71.1612\n",
      "Pre-training layer 0, epoch 30, cost  -71.0191\n",
      "Pre-training layer 0, epoch 31, cost  -70.9391\n",
      "Pre-training layer 0, epoch 32, cost  -70.8534\n",
      "Pre-training layer 0, epoch 33, cost  -70.7924\n",
      "Pre-training layer 0, epoch 34, cost  -70.7193\n",
      "Pre-training layer 0, epoch 35, cost  -70.6151\n",
      "Pre-training layer 0, epoch 36, cost  -70.5554\n",
      "Pre-training layer 0, epoch 37, cost  -70.4665\n",
      "Pre-training layer 0, epoch 38, cost  -70.3894\n",
      "Pre-training layer 0, epoch 39, cost  -70.3354\n",
      "Pre-training layer 0, epoch 40, cost  -70.2487\n",
      "Pre-training layer 0, epoch 41, cost  -70.1773\n",
      "Pre-training layer 0, epoch 42, cost  -70.1248\n",
      "Pre-training layer 0, epoch 43, cost  -70.0698\n",
      "Pre-training layer 0, epoch 44, cost  -70.0251\n",
      "Pre-training layer 0, epoch 45, cost  -69.9718\n",
      "Pre-training layer 0, epoch 46, cost  -69.9004\n",
      "Pre-training layer 0, epoch 47, cost  -69.8286\n",
      "Pre-training layer 0, epoch 48, cost  -69.7903\n",
      "Pre-training layer 0, epoch 49, cost  -69.7491\n",
      "Pre-training layer 0, epoch 50, cost  -69.7253\n",
      "Pre-training layer 0, epoch 51, cost  -69.6952\n",
      "Pre-training layer 0, epoch 52, cost  -69.62\n",
      "Pre-training layer 0, epoch 53, cost  -69.5747\n",
      "Pre-training layer 0, epoch 54, cost  -69.5587\n",
      "Pre-training layer 0, epoch 55, cost  -69.4855\n",
      "Pre-training layer 0, epoch 56, cost  -69.4881\n",
      "Pre-training layer 0, epoch 57, cost  -69.4373\n",
      "Pre-training layer 0, epoch 58, cost  -69.3915\n",
      "Pre-training layer 0, epoch 59, cost  -69.3721\n",
      "Pre-training layer 0, epoch 60, cost  -69.3391\n",
      "Pre-training layer 0, epoch 61, cost  -69.3037\n",
      "Pre-training layer 0, epoch 62, cost  -69.276\n",
      "Pre-training layer 0, epoch 63, cost  -69.2387\n",
      "Pre-training layer 0, epoch 64, cost  -69.2142\n",
      "Pre-training layer 0, epoch 65, cost  -69.1817\n",
      "Pre-training layer 0, epoch 66, cost  -69.1722\n",
      "Pre-training layer 0, epoch 67, cost  -69.1297\n",
      "Pre-training layer 0, epoch 68, cost  -69.0802\n",
      "Pre-training layer 0, epoch 69, cost  -69.0742\n",
      "Pre-training layer 0, epoch 70, cost  -69.0437\n",
      "Pre-training layer 0, epoch 71, cost  -69.018\n",
      "Pre-training layer 0, epoch 72, cost  -69.0019\n",
      "Pre-training layer 0, epoch 73, cost  -68.9949\n",
      "Pre-training layer 0, epoch 74, cost  -68.9514\n",
      "Pre-training layer 0, epoch 75, cost  -68.9239\n",
      "Pre-training layer 0, epoch 76, cost  -68.907\n",
      "Pre-training layer 0, epoch 77, cost  -68.9053\n",
      "Pre-training layer 0, epoch 78, cost  -68.8618\n",
      "Pre-training layer 0, epoch 79, cost  -68.8215\n",
      "Pre-training layer 0, epoch 80, cost  -68.8275\n",
      "Pre-training layer 0, epoch 81, cost  -68.8303\n",
      "Pre-training layer 0, epoch 82, cost  -68.7928\n",
      "Pre-training layer 0, epoch 83, cost  -68.7775\n",
      "Pre-training layer 0, epoch 84, cost  -68.7694\n",
      "Pre-training layer 0, epoch 85, cost  -68.7401\n",
      "Pre-training layer 0, epoch 86, cost  -68.7069\n",
      "Pre-training layer 0, epoch 87, cost  -68.7032\n",
      "Pre-training layer 0, epoch 88, cost  -68.6842\n",
      "Pre-training layer 0, epoch 89, cost  -68.6748\n",
      "Pre-training layer 0, epoch 90, cost  -68.6537\n",
      "Pre-training layer 0, epoch 91, cost  -68.6458\n",
      "Pre-training layer 0, epoch 92, cost  -68.6379\n",
      "Pre-training layer 0, epoch 93, cost  -68.6075\n",
      "Pre-training layer 0, epoch 94, cost  -68.5969\n",
      "Pre-training layer 0, epoch 95, cost  -68.5737\n",
      "Pre-training layer 0, epoch 96, cost  -68.5653\n",
      "Pre-training layer 0, epoch 97, cost  -68.555\n",
      "Pre-training layer 0, epoch 98, cost  -68.5199\n",
      "Pre-training layer 0, epoch 99, cost  -68.534\n",
      "Pre-training layer 1, epoch 0, cost  -174.546\n",
      "Pre-training layer 1, epoch 1, cost  -152.199\n",
      "Pre-training layer 1, epoch 2, cost  -147.102\n",
      "Pre-training layer 1, epoch 3, cost  -144.281\n",
      "Pre-training layer 1, epoch 4, cost  -142.388\n",
      "Pre-training layer 1, epoch 5, cost  -140.985\n",
      "Pre-training layer 1, epoch 6, cost  -139.838\n",
      "Pre-training layer 1, epoch 7, cost  -138.879\n",
      "Pre-training layer 1, epoch 8, cost  -138.113\n",
      "Pre-training layer 1, epoch 9, cost  -137.467\n",
      "Pre-training layer 1, epoch 10, cost  -136.905\n",
      "Pre-training layer 1, epoch 11, cost  -136.417\n",
      "Pre-training layer 1, epoch 12, cost  -135.979\n",
      "Pre-training layer 1, epoch 13, cost  -135.59\n",
      "Pre-training layer 1, epoch 14, cost  -135.283\n",
      "Pre-training layer 1, epoch 15, cost  -134.976\n",
      "Pre-training layer 1, epoch 16, cost  -134.69\n",
      "Pre-training layer 1, epoch 17, cost  -134.469\n",
      "Pre-training layer 1, epoch 18, cost  -134.232\n",
      "Pre-training layer 1, epoch 19, cost  -134.046\n",
      "Pre-training layer 1, epoch 20, cost  -133.868\n",
      "Pre-training layer 1, epoch 21, cost  -133.713\n",
      "Pre-training layer 1, epoch 22, cost  -133.551\n",
      "Pre-training layer 1, epoch 23, cost  -133.413\n",
      "Pre-training layer 1, epoch 24, cost  -133.295\n",
      "Pre-training layer 1, epoch 25, cost  -133.182\n",
      "Pre-training layer 1, epoch 26, cost  -133.067\n",
      "Pre-training layer 1, epoch 27, cost  -132.947\n",
      "Pre-training layer 1, epoch 28, cost  -132.875\n",
      "Pre-training layer 1, epoch 29, cost  -132.801\n",
      "Pre-training layer 1, epoch 30, cost  -132.714\n",
      "Pre-training layer 1, epoch 31, cost  -132.636\n",
      "Pre-training layer 1, epoch 32, cost  -132.556\n",
      "Pre-training layer 1, epoch 33, cost  -132.482\n",
      "Pre-training layer 1, epoch 34, cost  -132.425\n",
      "Pre-training layer 1, epoch 35, cost  -132.37\n",
      "Pre-training layer 1, epoch 36, cost  -132.291\n",
      "Pre-training layer 1, epoch 37, cost  -132.283\n",
      "Pre-training layer 1, epoch 38, cost  -132.221\n",
      "Pre-training layer 1, epoch 39, cost  -132.161\n",
      "Pre-training layer 1, epoch 40, cost  -132.107\n",
      "Pre-training layer 1, epoch 41, cost  -132.072\n",
      "Pre-training layer 1, epoch 42, cost  -132.029\n",
      "Pre-training layer 1, epoch 43, cost  -131.995\n",
      "Pre-training layer 1, epoch 44, cost  -131.943\n",
      "Pre-training layer 1, epoch 45, cost  -131.929\n",
      "Pre-training layer 1, epoch 46, cost  -131.879\n",
      "Pre-training layer 1, epoch 47, cost  -131.844\n",
      "Pre-training layer 1, epoch 48, cost  -131.835\n",
      "Pre-training layer 1, epoch 49, cost  -131.8\n",
      "Pre-training layer 1, epoch 50, cost  -131.79\n",
      "Pre-training layer 1, epoch 51, cost  -131.725\n",
      "Pre-training layer 1, epoch 52, cost  -131.703\n",
      "Pre-training layer 1, epoch 53, cost  -131.705\n",
      "Pre-training layer 1, epoch 54, cost  -131.693\n",
      "Pre-training layer 1, epoch 55, cost  -131.646\n",
      "Pre-training layer 1, epoch 56, cost  -131.618\n",
      "Pre-training layer 1, epoch 57, cost  -131.579\n",
      "Pre-training layer 1, epoch 58, cost  -131.592\n",
      "Pre-training layer 1, epoch 59, cost  -131.547\n",
      "Pre-training layer 1, epoch 60, cost  -131.545\n",
      "Pre-training layer 1, epoch 61, cost  -131.533\n",
      "Pre-training layer 1, epoch 62, cost  -131.501\n",
      "Pre-training layer 1, epoch 63, cost  -131.494\n",
      "Pre-training layer 1, epoch 64, cost  -131.48\n",
      "Pre-training layer 1, epoch 65, cost  -131.455\n",
      "Pre-training layer 1, epoch 66, cost  -131.438\n",
      "Pre-training layer 1, epoch 67, cost  -131.423\n",
      "Pre-training layer 1, epoch 68, cost  -131.392\n",
      "Pre-training layer 1, epoch 69, cost  -131.39\n",
      "Pre-training layer 1, epoch 70, cost  -131.385\n",
      "Pre-training layer 1, epoch 71, cost  -131.381\n",
      "Pre-training layer 1, epoch 72, cost  -131.355\n",
      "Pre-training layer 1, epoch 73, cost  -131.354\n",
      "Pre-training layer 1, epoch 74, cost  -131.356\n",
      "Pre-training layer 1, epoch 75, cost  -131.324\n",
      "Pre-training layer 1, epoch 76, cost  -131.318\n",
      "Pre-training layer 1, epoch 77, cost  -131.303\n",
      "Pre-training layer 1, epoch 78, cost  -131.308\n",
      "Pre-training layer 1, epoch 79, cost  -131.29\n",
      "Pre-training layer 1, epoch 80, cost  -131.277\n",
      "Pre-training layer 1, epoch 81, cost  -131.26\n",
      "Pre-training layer 1, epoch 82, cost  -131.278\n",
      "Pre-training layer 1, epoch 83, cost  -131.236\n",
      "Pre-training layer 1, epoch 84, cost  -131.225\n",
      "Pre-training layer 1, epoch 85, cost  -131.244\n",
      "Pre-training layer 1, epoch 86, cost  -131.207\n",
      "Pre-training layer 1, epoch 87, cost  -131.228\n",
      "Pre-training layer 1, epoch 88, cost  -131.216\n",
      "Pre-training layer 1, epoch 89, cost  -131.205\n",
      "Pre-training layer 1, epoch 90, cost  -131.165\n",
      "Pre-training layer 1, epoch 91, cost  -131.179\n",
      "Pre-training layer 1, epoch 92, cost  -131.165\n",
      "Pre-training layer 1, epoch 93, cost  -131.145\n",
      "Pre-training layer 1, epoch 94, cost  -131.164\n",
      "Pre-training layer 1, epoch 95, cost  -131.165\n",
      "Pre-training layer 1, epoch 96, cost  -131.132\n",
      "Pre-training layer 1, epoch 97, cost  -131.148\n",
      "Pre-training layer 1, epoch 98, cost  -131.134\n",
      "Pre-training layer 1, epoch 99, cost  -131.129\n",
      "Pre-training layer 2, epoch 0, cost  -69.1473\n",
      "Pre-training layer 2, epoch 1, cost  -57.0057\n",
      "Pre-training layer 2, epoch 2, cost  -54.623\n",
      "Pre-training layer 2, epoch 3, cost  -53.3244\n",
      "Pre-training layer 2, epoch 4, cost  -52.4889\n",
      "Pre-training layer 2, epoch 5, cost  -51.85\n",
      "Pre-training layer 2, epoch 6, cost  -51.3529\n",
      "Pre-training layer 2, epoch 7, cost  -50.9715\n",
      "Pre-training layer 2, epoch 8, cost  -50.6075\n",
      "Pre-training layer 2, epoch 9, cost  -50.2858\n",
      "Pre-training layer 2, epoch 10, cost  -50.0366\n",
      "Pre-training layer 2, epoch 11, cost  -49.7865\n",
      "Pre-training layer 2, epoch 12, cost  -49.5624\n",
      "Pre-training layer 2, epoch 13, cost  -49.3638\n",
      "Pre-training layer 2, epoch 14, cost  -49.2085\n",
      "Pre-training layer 2, epoch 15, cost  -49.0635\n",
      "Pre-training layer 2, epoch 16, cost  -48.9334\n",
      "Pre-training layer 2, epoch 17, cost  -48.7689\n",
      "Pre-training layer 2, epoch 18, cost  -48.6917\n",
      "Pre-training layer 2, epoch 19, cost  -48.5403\n",
      "Pre-training layer 2, epoch 20, cost  -48.4226\n",
      "Pre-training layer 2, epoch 21, cost  -48.3436\n",
      "Pre-training layer 2, epoch 22, cost  -48.261\n",
      "Pre-training layer 2, epoch 23, cost  -48.1824\n",
      "Pre-training layer 2, epoch 24, cost  -48.0689\n",
      "Pre-training layer 2, epoch 25, cost  -47.9816\n",
      "Pre-training layer 2, epoch 26, cost  -47.9351\n",
      "Pre-training layer 2, epoch 27, cost  -47.8458\n",
      "Pre-training layer 2, epoch 28, cost  -47.7894\n",
      "Pre-training layer 2, epoch 29, cost  -47.729\n",
      "Pre-training layer 2, epoch 30, cost  -47.665\n",
      "Pre-training layer 2, epoch 31, cost  -47.5874\n",
      "Pre-training layer 2, epoch 32, cost  -47.5215\n",
      "Pre-training layer 2, epoch 33, cost  -47.4883\n",
      "Pre-training layer 2, epoch 34, cost  -47.4103\n",
      "Pre-training layer 2, epoch 35, cost  -47.4074\n",
      "Pre-training layer 2, epoch 36, cost  -47.3477\n",
      "Pre-training layer 2, epoch 37, cost  -47.3193\n",
      "Pre-training layer 2, epoch 38, cost  -47.2308\n",
      "Pre-training layer 2, epoch 39, cost  -47.2032\n",
      "Pre-training layer 2, epoch 40, cost  -47.1865\n",
      "Pre-training layer 2, epoch 41, cost  -47.1342\n",
      "Pre-training layer 2, epoch 42, cost  -47.0919\n",
      "Pre-training layer 2, epoch 43, cost  -47.0585\n",
      "Pre-training layer 2, epoch 44, cost  -47.0425\n",
      "Pre-training layer 2, epoch 45, cost  -47.0217\n",
      "Pre-training layer 2, epoch 46, cost  -46.9772\n",
      "Pre-training layer 2, epoch 47, cost  -46.9527\n",
      "Pre-training layer 2, epoch 48, cost  -46.9266\n",
      "Pre-training layer 2, epoch 49, cost  -46.9038\n",
      "Pre-training layer 2, epoch 50, cost  -46.8621\n",
      "Pre-training layer 2, epoch 51, cost  -46.8387\n",
      "Pre-training layer 2, epoch 52, cost  -46.8259\n",
      "Pre-training layer 2, epoch 53, cost  -46.7787\n",
      "Pre-training layer 2, epoch 54, cost  -46.7842\n",
      "Pre-training layer 2, epoch 55, cost  -46.7555\n",
      "Pre-training layer 2, epoch 56, cost  -46.722\n",
      "Pre-training layer 2, epoch 57, cost  -46.6894\n",
      "Pre-training layer 2, epoch 58, cost  -46.6536\n",
      "Pre-training layer 2, epoch 59, cost  -46.6607\n",
      "Pre-training layer 2, epoch 60, cost  -46.6352\n",
      "Pre-training layer 2, epoch 61, cost  -46.6158\n",
      "Pre-training layer 2, epoch 62, cost  -46.5971\n",
      "Pre-training layer 2, epoch 63, cost  -46.5997\n",
      "Pre-training layer 2, epoch 64, cost  -46.5634\n",
      "Pre-training layer 2, epoch 65, cost  -46.5112\n",
      "Pre-training layer 2, epoch 66, cost  -46.5302\n",
      "Pre-training layer 2, epoch 67, cost  -46.5126\n",
      "Pre-training layer 2, epoch 68, cost  -46.5049\n",
      "Pre-training layer 2, epoch 69, cost  -46.4638\n",
      "Pre-training layer 2, epoch 70, cost  -46.4483\n",
      "Pre-training layer 2, epoch 71, cost  -46.4482\n",
      "Pre-training layer 2, epoch 72, cost  -46.4328\n",
      "Pre-training layer 2, epoch 73, cost  -46.426\n",
      "Pre-training layer 2, epoch 74, cost  -46.3955\n",
      "Pre-training layer 2, epoch 75, cost  -46.3901\n",
      "Pre-training layer 2, epoch 76, cost  -46.3683\n",
      "Pre-training layer 2, epoch 77, cost  -46.3625\n",
      "Pre-training layer 2, epoch 78, cost  -46.3556\n",
      "Pre-training layer 2, epoch 79, cost  -46.3489\n",
      "Pre-training layer 2, epoch 80, cost  -46.3035\n",
      "Pre-training layer 2, epoch 81, cost  -46.3269\n",
      "Pre-training layer 2, epoch 82, cost  -46.3094\n",
      "Pre-training layer 2, epoch 83, cost  -46.2903\n",
      "Pre-training layer 2, epoch 84, cost  -46.2822\n",
      "Pre-training layer 2, epoch 85, cost  -46.2899\n",
      "Pre-training layer 2, epoch 86, cost  -46.2541\n",
      "Pre-training layer 2, epoch 87, cost  -46.2635\n",
      "Pre-training layer 2, epoch 88, cost  -46.2388\n",
      "Pre-training layer 2, epoch 89, cost  -46.2241\n",
      "Pre-training layer 2, epoch 90, cost  -46.209\n",
      "Pre-training layer 2, epoch 91, cost  -46.2258\n",
      "Pre-training layer 2, epoch 92, cost  -46.2081\n",
      "Pre-training layer 2, epoch 93, cost  -46.201\n",
      "Pre-training layer 2, epoch 94, cost  -46.1909\n",
      "Pre-training layer 2, epoch 95, cost  -46.1974\n",
      "Pre-training layer 2, epoch 96, cost  -46.1736\n",
      "Pre-training layer 2, epoch 97, cost  -46.1609\n",
      "Pre-training layer 2, epoch 98, cost  -46.1753\n",
      "Pre-training layer 2, epoch 99, cost "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mod.cu\r\n",
      "   C:/Users/nuri/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.9-64/tmpp0xd5o/bc940197df4cf7ff39e35b62244ce393.lib ���̺귯�� �� C:/Users/nuri/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.9-64/tmpp0xd5o/bc940197df4cf7ff39e35b62244ce393.exp ��ü�� �����ϰ� �ֽ��ϴ�.\r\n",
      "\n",
      "The pretraining code for file DBN.py ran for 77.11m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -46.1407\n",
      "... getting the finetuning functions\n",
      "... finetuning the model\n",
      "epoch 1, minibatch 5000/5000, validation error 3.070000 %\n",
      "     epoch 1, minibatch 5000/5000, test error of best model 3.520000 %\n",
      "epoch 2, minibatch 5000/5000, validation error 2.520000 %\n",
      "     epoch 2, minibatch 5000/5000, test error of best model 2.780000 %\n",
      "epoch 3, minibatch 5000/5000, validation error 2.230000 %\n",
      "     epoch 3, minibatch 5000/5000, test error of best model 2.450000 %\n",
      "epoch 4, minibatch 5000/5000, validation error 2.000000 %\n",
      "     epoch 4, minibatch 5000/5000, test error of best model 2.260000 %\n",
      "epoch 5, minibatch 5000/5000, validation error 1.980000 %\n",
      "     epoch 5, minibatch 5000/5000, test error of best model 1.990000 %\n",
      "epoch 6, minibatch 5000/5000, validation error 1.850000 %\n",
      "     epoch 6, minibatch 5000/5000, test error of best model 1.840000 %\n",
      "epoch 7, minibatch 5000/5000, validation error 1.770000 %\n",
      "     epoch 7, minibatch 5000/5000, test error of best model 1.780000 %\n",
      "epoch 8, minibatch 5000/5000, validation error 1.700000 %\n",
      "     epoch 8, minibatch 5000/5000, test error of best model 1.730000 %\n",
      "epoch 9, minibatch 5000/5000, validation error 1.680000 %\n",
      "     epoch 9, minibatch 5000/5000, test error of best model 1.740000 %\n",
      "epoch 10, minibatch 5000/5000, validation error 1.640000 %\n",
      "     epoch 10, minibatch 5000/5000, test error of best model 1.690000 %\n",
      "epoch 11, minibatch 5000/5000, validation error 1.580000 %\n",
      "     epoch 11, minibatch 5000/5000, test error of best model 1.660000 %\n",
      "epoch 12, minibatch 5000/5000, validation error 1.600000 %\n",
      "epoch 13, minibatch 5000/5000, validation error 1.540000 %\n",
      "     epoch 13, minibatch 5000/5000, test error of best model 1.670000 %\n",
      "epoch 14, minibatch 5000/5000, validation error 1.500000 %\n",
      "     epoch 14, minibatch 5000/5000, test error of best model 1.610000 %\n",
      "epoch 15, minibatch 5000/5000, validation error 1.520000 %\n",
      "epoch 16, minibatch 5000/5000, validation error 1.510000 %\n",
      "epoch 17, minibatch 5000/5000, validation error 1.520000 %\n",
      "epoch 18, minibatch 5000/5000, validation error 1.520000 %\n",
      "epoch 19, minibatch 5000/5000, validation error 1.530000 %\n",
      "epoch 20, minibatch 5000/5000, validation error 1.540000 %\n",
      "epoch 21, minibatch 5000/5000, validation error 1.550000 %\n",
      "epoch 22, minibatch 5000/5000, validation error 1.540000 %\n",
      "epoch 23, minibatch 5000/5000, validation error 1.520000 %\n",
      "epoch 24, minibatch 5000/5000, validation error 1.540000 %\n",
      "epoch 25, minibatch 5000/5000, validation error 1.530000 %\n",
      "epoch 26, minibatch 5000/5000, validation error 1.530000 %\n",
      "epoch 27, minibatch 5000/5000, validation error 1.520000 %\n",
      "Optimization complete with best validation score of 1.500000 %, obtained at iteration 70000, with test performance 1.610000 %"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "The fine tuning code for file DBN.py ran for 7.72m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import DBN\n",
    "DBN.test_DBN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
